{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diferenciacion automatica\n",
    "\n",
    "Supongamos que necesitamos encontrar derivadas de la funcion $F(x)=(5x+1)^2$\n",
    "\n",
    "Si descomponemos la funci\\'on $F(x)=f(g(x))$, donde $f(x)=x^2$ y $g(x)=5x+1$ entonces podemos calcular la derivada de $\\frac{\\partial F(x)}{\\partial x}$ como:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial F(x)}{\\partial x}=\\frac{\\partial f(g(x))}{\\partial x} \\times \\frac{\\partial g(x)}{\\partial x}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F(X) : 1.000000\n",
      "dF/dx : 10.000000\n"
     ]
    }
   ],
   "source": [
    "from autograd import grad\n",
    "from autograd.core import primitive\n",
    "import autograd.numpy as np\n",
    "\n",
    "def F(x):\n",
    "    return (5*x+1)**2\n",
    "\n",
    "grad_F = grad(F)\n",
    "\n",
    "print ('F(X) : %.6f'%F(0.0))\n",
    "print ('dF/dx : %.6f'%grad_F(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales\n",
    "\n",
    "Ahora en el contexto del aprendizaje automatico, queremos eoncontrar derivadas parciales de una funcion objetivo con respecto a los parametros del modelo. En el caso de problemas de clasificacion binaria multi-variable contamos con datos de entrada $\\textbf X=(\\mathbf x_1,\\ldots,\\mathbf x_n)$ y sus correspondientes etiquetas $\\textbf y=(y_1,\\ldots,y_n)$.\n",
    "\n",
    "Para cada tupla $(x_i,y_i)$ podemos establecer la siguiente relacion lineal:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{log}\\left(\\frac{p( C=1 \\vert \\mathbf x_i )}{p(C=0 \\vert \\mathbf x_i )} \\right)&=w_0+\\sum_{j=1}^{n} w_j\\,x_{ij}\\\\\n",
    "&=\\mathbf w^T \\mathbf x_i\n",
    "\\end{align}  \n",
    "\n",
    "Al mismo tiempo , podemos obtener las probabilidades condicionales para cada clase:\n",
    "\n",
    "\\begin{align}\n",
    "p(C=1\\vert \\textbf x_i)&= \\phi(\\mathbf w^T \\mathbf x_i)=\\frac{\\text{exp}(\\mathbf w^T \\mathbf x_i)}{1+\\text{exp}(\\mathbf w^T \\mathbf x_i)}\n",
    "\\end{align} \n",
    "\n",
    "Donde $\\phi(\\cdot)$ es la llamada funciÃ³n sigmoide.\n",
    "\n",
    "La funcion sigmoide convierte cualquier entrada $\\mathbf w^T \\mathbf x_i \\in [-\\inf,\\inf]$ hacia el intervalo $(0,1)$, por lo tanto podemos utilizar la funcion de verosimilitud de la distribucion de Bernoulli para comparar las probabilidad de clase con las etiquetas binarias:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\textbf y, \\textbf X,\\mathbf w)= \\prod_{i=1}^N (\\phi(\\mathbf w^T \\mathbf x_i))^y_i \\times (1-\\phi(\\mathbf w^T \\mathbf x_i))^{1-y_i}\n",
    "\\end{align}\n",
    "\n",
    "Ahora queremos obtener los parametros $\\hat{\\mathbf w}$ que maximicen la verosimilitud $p(\\textbf y, \\textbf X,\\mathbf w)$. Dado que la funcion logaritmo es monotona en todo el dominio $[-\\inf,\\inf]$ utilizamos $\\operatorname{log} p(\\textbf y, \\textbf X,\\mathbf w)$ como funcion objetivo (log-loss):\n",
    "\n",
    "\\begin{align}\n",
    "L(\\mathbf w)=\\sum_{i=1}^N (y_i \\operatorname{log} \\phi(\\mathbf w^T \\mathbf x_i)) + ({1-y_i}) \\operatorname{log} (1-\\phi(\\mathbf w^T \\mathbf x_i))\n",
    "\\end{align}\n",
    "\n",
    "Ahora que tenemos definida una funcion objetivo, podemos evaluar iterativamente los pesos $w$ escogiendo la direccion de maximo crecimiento (gradiente).\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf w^{j+1}= \\mathbf  w^j-\\eta \\frac{\\partial L(\\mathbf w)}{\\partial \\mathbf w}\n",
    "\\end{align}\n",
    "\n",
    "Donde $\\eta > 0$ es un factor de descuento o tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from builtins import range\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from autograd.test_util import check_grads\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return np.exp(x)/(1+np.exp(x))\n",
    "\n",
    "def logistic_predictions(weights, inputs):\n",
    "    # Outputs probability of a label being true according to logistic model.\n",
    "    return sigmoid(np.dot(inputs, weights))\n",
    "\n",
    "def training_loss(weights):\n",
    "    # Training loss is the negative log-likelihood of the training labels.\n",
    "    preds = logistic_predictions(weights, inputs)\n",
    "    label_probabilities = np.log(preds) * targets + np.log((1 - preds)) * (1 - targets)\n",
    "    return -np.sum(label_probabilities)\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = np.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = np.array([True, True, False, True])\n",
    "\n",
    "# Build a function that returns gradients of training loss using autograd.\n",
    "training_gradient_fun = grad(training_loss)\n",
    "\n",
    "# Check the gradients numerically, just to be safe.\n",
    "weights = np.array([0.0, 0.0, 0.0])\n",
    "check_grads(training_loss, modes=['rev'])(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0, Trained loss: 2.283497754191519\n",
      "Iteration : 100, Trained loss: 0.16583313868587354\n",
      "Iteration : 200, Trained loss: 0.08498108937659068\n",
      "Iteration : 300, Trained loss: 0.05693670361850756\n",
      "Iteration : 400, Trained loss: 0.04275843168296287\n",
      "Iteration : 500, Trained loss: 0.03421470813642524\n",
      "Iteration : 600, Trained loss: 0.028508088444567432\n",
      "Iteration : 700, Trained loss: 0.024428500857397714\n",
      "Iteration : 800, Trained loss: 0.021367810994468505\n",
      "Iteration : 900, Trained loss: 0.01898713850700267\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history=[]\n",
    "eta=0.1\n",
    "iterations=1000\n",
    "for i in range(iterations):\n",
    "    weights -= training_gradient_fun(weights) * eta\n",
    "    loss_history.append(training_loss(weights))\n",
    "    if i%(iterations/10)==0:\n",
    "        print(\"Iteration : {0}, Trained loss: {1}\".format(i,training_loss(weights)))\n",
    "        \n",
    "\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from  sklearn import  datasets\n",
    "iris=datasets.load_iris()\n",
    "x=iris.data\n",
    "y=iris.target\n",
    "\n",
    "y=y[iris.target!=2]\n",
    "x=x[iris.target!=2,:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.5)\n",
    "from sklearn import tree\n",
    "classifier=tree.DecisionTreeClassifier()\n",
    "classifier.fit(x_train,y_train)\n",
    "predictions=classifier.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([0.0, 0.0, 0.0,0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
